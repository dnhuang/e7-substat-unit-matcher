{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Epic Seven Character Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I start by scraping character names. Once again, using the [Regex101](https://regex101.com/) made the endeavor quite simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Achates', 'Adin', 'Adlay', 'Adventurer Ras', 'Ainos']\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "name_url = 'https://epic7x.com/characters/'\n",
    "\n",
    "# API Call\n",
    "name_response_string = requests.get(name_url).text\n",
    "\n",
    "# Get name of all current units\n",
    "name_pattern = \"\\\"name\\\":\\\"([\\w ]+)\\\",\\\"icon\\\"\"\n",
    "name_list = re.findall(name_pattern, name_response_string)\n",
    "\n",
    "# Display check\n",
    "print(name_list[:5])\n",
    "print(len(name_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 4/17/2023, there are a total of $281$ characters in Epic Seven, so it the extraction was successful. Moving on to each individual character page:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In name list, whitespace characters are used. Need to replace with `\"+\"` to match unit URL."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Scrape - Success"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are the intermediate code I used to build up my multi-scrape function, but this is also where an interesting issue manifests. Below is an example of a successful scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dystatechange=function(){var t;4==s.readyState&&(200<=s.status&&s.status<300||304==s.status)&&((t=e.createElement(\"script\")).type=\"text/javascript\",t.text=s.responseText,e.head.appendChild(t))},s.send(null)}(document);\n",
      "        (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n",
      "        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\n",
      "        'https://www.googletagmanager.com/gtm.js?id='+\n"
     ]
    }
   ],
   "source": [
    "response_check = requests.get(\"https://epic7x.com/character/angelica/\").text\n",
    "print(response_check[1500:2000]) # error message around 1500:2000 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[\"Hp%</b></span>,                                             <!-- <span class='f-16'><b>Def%</b></span>  <img class='lazyloaded character-rarity ml-15' src='https://epic7x.com/wp-content/themes/epic7x/assets/img/arrow.png'>  -->\\n                        <span class='f-16'><b>Def%</b></span>,                                             <!-- <span class='f-16'><b>Speed</b></span>  <img class='lazyloaded character-rarity ml-15' src='https://epic7x.com/wp-content/themes/epic7x/assets/img/arrow.png'>  -->\\n                        <span class='f-16'><b>Speed</b></span>,                                             <!-- <span class='f-16'><b>Effect Resistance%</b></span>  -->\\n                        <span class='f-16'><b>Effect Resistance%</b></span>                                        </div>\\n                \"]\n"
     ]
    }
   ],
   "source": [
    "substats_blob_pattern = \"<span class='f-16'><b>([\\w\\W]{0,900})<\\/div>\"\n",
    "substats_blob_list = re.findall(substats_blob_pattern, response_check)\n",
    "print(len(substats_blob_list))\n",
    "print(substats_blob_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hp%', 'Def%', 'Speed', 'Effect Resistance%']\n"
     ]
    }
   ],
   "source": [
    "split_list = substats_blob_list[0].split('<b>')\n",
    "stat_list = []\n",
    "for stat_blob in split_list:\n",
    "    stat = stat_blob.split(\"</b>\")[0] # 0th element is the stat\n",
    "    #print(stat) # TYPE str lit\n",
    "    if stat not in stat_list: # don't add dupes\n",
    "        stat_list.append(stat)\n",
    "print(stat_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Scrape - Fail"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is code for a failed scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".net/wordpress-speed/wpx-hosting-promo-coupon-discount-code/\" target=\"_blank\"><img src=\"https://cf.wpx.net/img/No-Website-Installed.png\" style=\"width: 450px;\" /></a>\n",
      "        <br />\n",
      "        &nbsp;\n",
      "        <br />\n",
      "        <span style=\"font-weight: 500;\">You have reached an error page on WPX.net</span>\n",
      "        <br />\n",
      "        &nbsp;\n",
      "        <br />\n",
      "        If you are seeing this page, it is because there is no website installed on this domain yet.\n",
      "        <br />\n",
      "        &nbsp;\n",
      "        <br /\n"
     ]
    }
   ],
   "source": [
    "response_check = requests.get(\"https://epic7x.com/character/assassin-cidd/\").text\n",
    "print(response_check[1500:2000]) # error message around 1500:2000 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "substats_blob_pattern = \"<span class='f-16'><b>([\\w\\W]{0,900})<\\/div>\"\n",
    "substats_blob_list = re.findall(substats_blob_pattern, response_check)\n",
    "print(len(substats_blob_list))\n",
    "print(substats_blob_list[0:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Scrape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite knowing failures exist, I implemented a Python function for scraping the entire character database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_character_substats(name_list):\n",
    "    # Helper function to extract unit\n",
    "    def extract_substats(name):\n",
    "        # attempt at trying to get around scrape blocker\n",
    "        # wait_time = random.randrange(1, 5, 1)\n",
    "        # time.sleep(wait_time)\n",
    "        # print('Time waited: {}'.format(wait_time))\n",
    "        \n",
    "        # print(name) # DEBUG\n",
    "\n",
    "        # create correct URL\n",
    "        name_cleaned = name.replace(\" \", \"-\").replace(\"\\'\", \"\").lower()\n",
    "        name_url = \"https://epic7x.com/character/\" + name_cleaned + \"/\"\n",
    "        # print(name_url) # DEBUG\n",
    "        substats_blob_response_string = requests.get(name_url).text # API call\n",
    "        # print(substats_blob_response_string[:100]) # DEBUG\n",
    "        # print(substats_blob_response_string) # DEBUG\n",
    "\n",
    "        # instantiate pattern\n",
    "        substats_blob_pattern = \"<span class='f-16'><b>([\\w\\W]{0,1300})<\\/div>\"\n",
    "        substats_blob_list = re.findall(substats_blob_pattern, substats_blob_response_string)\n",
    "\n",
    "        # split `substats_blob_list` so that 0th index is the substat\n",
    "        try: \n",
    "            split_substats_list = substats_blob_list[0].split('<b>')\n",
    "        except:\n",
    "            # print(\"Blocked, scraping next character.\") # DEBUG\n",
    "            # print(\"<------------------------->\\n\") # DEBUG\n",
    "            return [False, []] # extract failed, return false and empty\n",
    "\n",
    "        # extract substats from `split_substats_list`, 0th element \n",
    "        substats_list = []\n",
    "        for stat_blob in split_substats_list:\n",
    "            stat = stat_blob.split(\"</b>\")[0] # 0th element is the stat\n",
    "            #print(stat) # TYPE str lit\n",
    "            if stat not in substats_list: # don't add dupes\n",
    "                substats_list.append(stat)\n",
    "\n",
    "        # display checks            \n",
    "        # print(substats_list) # DEBUG\n",
    "        # print(\"<------------------------->\\n\") # DEBUG\n",
    "        return [True, (name, substats_list)] # extract successs, list containing true and unit tuple\n",
    "\n",
    "    ### START OF PARENT FUNCTION ###\n",
    "    # instatiate variables for statistic display\n",
    "    num_success = 0\n",
    "    num_fail = 0\n",
    "    num_requests = 0\n",
    "\n",
    "    # instantiate character list\n",
    "    character_list = []\n",
    "\n",
    "    # perfrom extraction\n",
    "    for name in name_list:\n",
    "        if name != \"Architect Laika\":\n",
    "            # 0th index is true/false, 1st index is the unit-substats tuple\n",
    "            bool_unit_pair = extract_substats(name)\n",
    "            is_success = bool_unit_pair[0]\n",
    "            if (is_success):\n",
    "                num_success += 1\n",
    "                # add to character list\n",
    "                character_list.append(bool_unit_pair[1])\n",
    "            else:\n",
    "                num_fail += 1\n",
    "    \n",
    "            num_requests += 1\n",
    "\n",
    "    # display extraction stats\n",
    "    print(\"num requests: {}\".format(num_requests))\n",
    "    print(\"num success: {}\".format(num_success))\n",
    "    print(\"num fail: {}\".format(num_fail))\n",
    "    print(\"success rate: {:.4f}\".format(num_success/num_requests))\n",
    "    return character_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num requests: 280\n",
      "num success: 71\n",
      "num fail: 209\n",
      "success rate: 0.2536\n"
     ]
    }
   ],
   "source": [
    "extracted_character_list = extract_all_character_substats(name_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite certain character pages getting blocked, I still scraped what I could. With a roughly $25\\%$ success rate, I don't think it's too bad, but it's not nearly enough to create a useful tool for me to use. In my README I mentioned a cheap tool, but it needs to be *usable*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue - Data Scraping Block\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `robot.txt` for [Epic7x]()\n",
    "\n",
    "<div>\n",
    "    <a href=\"https://epic7x.com/robots.txt\">\n",
    "        <img src='img/epic7x_robot.png' width=500>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not a web developer, but from Googling around and reading [this article](https://www.codementor.io/@scrapingdog/10-tips-to-avoid-getting-blocked-while-scraping-websites-16papipe62?fbclid=IwAR0Cv8zD98Efq-R5V04FCDiOpYPJK7eC9oRGTUVkK35DlP7VSdRo1KK0X9s), I believe there are some mechanisms in place that are stopping me from scraping the character pages.\n",
    "\n",
    "I tried following some of the tips listed in the article to try to get around this issue, like instilling a random wait time in-between page requests and using a different header, but to no avail. I also didn't want to bother changing my IP address or doing VPN related things as that's outside the realm of I'm interested in (also too lazy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
