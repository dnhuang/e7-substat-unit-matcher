{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Epic Seven Character Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I start by scraping character names. Once again, using the [Regex101](https://regex101.com/) made the endeavor quite simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Achates', 'Adin', 'Adlay', 'Adventurer Ras', 'Ainos']\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "name_url = 'https://epic7x.com/characters/'\n",
    "\n",
    "# API Call\n",
    "name_response_string = requests.get(name_url).text\n",
    "\n",
    "# Get name of all current units\n",
    "name_pattern = \"\\\"name\\\":\\\"([\\w ]+)\\\",\\\"icon\\\"\"\n",
    "name_list = re.findall(name_pattern, name_response_string)\n",
    "\n",
    "# Display check\n",
    "print(name_list[:5])\n",
    "print(len(name_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of 4/17/2023, there are a total of $281$ characters in Epic Seven, so it the extraction was successful. Moving on to each individual character page:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In name list, whitespace characters are used. Need to replace with `\"+\"` to match unit URL."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Scrape - Success"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells are the intermediate code I used to build up my multi-scrape function, but this is also where an interesting issue manifests. Below is an example of a successful scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dystatechange=function(){var t;4==s.readyState&&(200<=s.status&&s.status<300||304==s.status)&&((t=e.createElement(\"script\")).type=\"text/javascript\",t.text=s.responseText,e.head.appendChild(t))},s.send(null)}(document);\n",
      "        (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':\n",
      "        new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],\n",
      "        j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=\n",
      "        'https://www.googletagmanager.com/gtm.js?id='+\n"
     ]
    }
   ],
   "source": [
    "response_check = requests.get(\"https://epic7x.com/character/angelica/\").text\n",
    "print(response_check[1500:2000]) # error message around 1500:2000 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[\"Hp%</b></span>,                                             <!-- <span class='f-16'><b>Def%</b></span>  <img class='lazyloaded character-rarity ml-15' src='https://epic7x.com/wp-content/themes/epic7x/assets/img/arrow.png'>  -->\\n                        <span class='f-16'><b>Def%</b></span>,                                             <!-- <span class='f-16'><b>Speed</b></span>  <img class='lazyloaded character-rarity ml-15' src='https://epic7x.com/wp-content/themes/epic7x/assets/img/arrow.png'>  -->\\n                        <span class='f-16'><b>Speed</b></span>,                                             <!-- <span class='f-16'><b>Effect Resistance%</b></span>  -->\\n                        <span class='f-16'><b>Effect Resistance%</b></span>                                        </div>\\n                \"]\n"
     ]
    }
   ],
   "source": [
    "substats_blob_pattern = \"<span class='f-16'><b>([\\w\\W]{0,900})<\\/div>\"\n",
    "substats_blob_list = re.findall(substats_blob_pattern, response_check)\n",
    "print(len(substats_blob_list))\n",
    "print(substats_blob_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hp%', 'Def%', 'Speed', 'Effect Resistance%']\n"
     ]
    }
   ],
   "source": [
    "split_list = substats_blob_list[0].split('<b>')\n",
    "stat_list = []\n",
    "for stat_blob in split_list:\n",
    "    stat = stat_blob.split(\"</b>\")[0] # 0th element is the stat\n",
    "    #print(stat) # TYPE str lit\n",
    "    if stat not in stat_list: # don't add dupes\n",
    "        stat_list.append(stat)\n",
    "print(stat_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Scrape - Fail"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is code for a failed scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".net/wordpress-speed/wpx-hosting-promo-coupon-discount-code/\" target=\"_blank\"><img src=\"https://cf.wpx.net/img/No-Website-Installed.png\" style=\"width: 450px;\" /></a>\n",
      "        <br />\n",
      "        &nbsp;\n",
      "        <br />\n",
      "        <span style=\"font-weight: 500;\">You have reached an error page on WPX.net</span>\n",
      "        <br />\n",
      "        &nbsp;\n",
      "        <br />\n",
      "        If you are seeing this page, it is because there is no website installed on this domain yet.\n",
      "        <br />\n",
      "        &nbsp;\n",
      "        <br /\n"
     ]
    }
   ],
   "source": [
    "response_check = requests.get(\"https://epic7x.com/character/assassin-cidd/\").text\n",
    "print(response_check[1500:2000]) # error message around 1500:2000 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "substats_blob_pattern = \"<span class='f-16'><b>([\\w\\W]{0,900})<\\/div>\"\n",
    "substats_blob_list = re.findall(substats_blob_pattern, response_check)\n",
    "print(len(substats_blob_list))\n",
    "print(substats_blob_list[0:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Scrape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite knowing failures exist, I implemented a Python function for scraping the entire character database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_character_substats(name_list):\n",
    "    # Helper function to extract unit\n",
    "    def extract_substats(name):\n",
    "        # attempt at trying to get around scrape blocker\n",
    "        # wait_time = random.randrange(1, 5, 1)\n",
    "        # time.sleep(wait_time)\n",
    "        # print('Time waited: {}'.format(wait_time))\n",
    "        \n",
    "        # print(name) # DEBUG\n",
    "\n",
    "        # create correct URL\n",
    "        name_cleaned = name.replace(\" \", \"-\").replace(\"\\'\", \"\").lower()\n",
    "        name_url = \"https://epic7x.com/character/\" + name_cleaned + \"/\"\n",
    "        # print(name_url) # DEBUG\n",
    "        substats_blob_response_string = requests.get(name_url).text # API call\n",
    "        # print(substats_blob_response_string[:100]) # DEBUG\n",
    "        # print(substats_blob_response_string) # DEBUG\n",
    "\n",
    "        # instantiate pattern\n",
    "        substats_blob_pattern = \"<span class='f-16'><b>([\\w\\W]{0,1300})<\\/div>\"\n",
    "        substats_blob_list = re.findall(substats_blob_pattern, substats_blob_response_string)\n",
    "\n",
    "        # split `substats_blob_list` so that 0th index is the substat\n",
    "        try: \n",
    "            split_substats_list = substats_blob_list[0].split('<b>')\n",
    "        except:\n",
    "            # print(\"Blocked, scraping next character.\") # DEBUG\n",
    "            # print(\"<------------------------->\\n\") # DEBUG\n",
    "            return [False, []] # extract failed, return false and empty\n",
    "\n",
    "        # extract substats from `split_substats_list`, 0th element \n",
    "        substats_list = []\n",
    "        for stat_blob in split_substats_list:\n",
    "            stat = stat_blob.split(\"</b>\")[0] # 0th element is the stat\n",
    "            #print(stat) # TYPE str lit\n",
    "            if stat not in substats_list: # don't add dupes\n",
    "                substats_list.append(stat)\n",
    "\n",
    "        # display checks            \n",
    "        # print(substats_list) # DEBUG\n",
    "        # print(\"<------------------------->\\n\") # DEBUG\n",
    "        return [True, (name, substats_list)] # extract successs, list containing true and unit tuple\n",
    "\n",
    "    ### START OF PARENT FUNCTION ###\n",
    "    # instatiate variables for statistic display\n",
    "    num_success = 0\n",
    "    num_fail = 0\n",
    "    num_requests = 0\n",
    "\n",
    "    # instantiate character list\n",
    "    character_list = []\n",
    "\n",
    "    # perfrom extraction\n",
    "    for name in name_list:\n",
    "        if name != \"Architect Laika\":\n",
    "            # 0th index is true/false, 1st index is the unit-substats tuple\n",
    "            bool_unit_pair = extract_substats(name)\n",
    "            is_success = bool_unit_pair[0]\n",
    "            if (is_success):\n",
    "                num_success += 1\n",
    "                # add to character list\n",
    "                character_list.append(bool_unit_pair[1])\n",
    "            else:\n",
    "                num_fail += 1\n",
    "    \n",
    "            num_requests += 1\n",
    "\n",
    "    # display extraction stats\n",
    "    print(\"num requests: {}\".format(num_requests))\n",
    "    print(\"num success: {}\".format(num_success))\n",
    "    print(\"num fail: {}\".format(num_fail))\n",
    "    print(\"success rate: {:.4f}\".format(num_success/num_requests))\n",
    "    return character_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num requests: 280\n",
      "num success: 82\n",
      "num fail: 198\n",
      "success rate: 0.2929\n"
     ]
    }
   ],
   "source": [
    "extracted_character_list = extract_all_character_substats(name_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite certain character pages getting blocked, I still scraped what I could. With success rate floating around the $20\\%-30\\%$ rate, I don't think it's too bad, but it's not nearly enough to create a useful tool for me to use. In my README I mentioned a cheap tool, but it needs to be *usable*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue - Data Scraping Block\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `robot.txt` for [Epic7x]()\n",
    "\n",
    "<div>\n",
    "    <a href=\"https://epic7x.com/robots.txt\">\n",
    "        <img src='img/epic7x_robot.png' width=500>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not a web developer, but from Googling around and reading [this article](https://www.codementor.io/@scrapingdog/10-tips-to-avoid-getting-blocked-while-scraping-websites-16papipe62?fbclid=IwAR0Cv8zD98Efq-R5V04FCDiOpYPJK7eC9oRGTUVkK35DlP7VSdRo1KK0X9s), I believe there are some mechanisms in place that are stopping me from scraping the character pages.\n",
    "\n",
    "I tried following some of the tips listed in the article to try to get around this issue, like instilling a random wait time in-between page requests and using a different header, but to no avail. I also didn't want to bother changing my IP address or doing VPN related things as that's outside the realm of I'm interested in (also too lazy)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Extracted Data into a `.csv` File"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a closer look at an element `extracted_character_list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Adin', ['Atk%', 'Crit Damage%', 'Crit Rate%', 'Speed']),\n",
       " ('Adlay', ['Atk%', 'Crit Rate%', 'Crit Damage%', 'Speed']),\n",
       " ('Adventurer Ras', ['Hp%', 'Speed', 'Def%', 'Effectiveness%']),\n",
       " ('Ainos', ['Speed', 'Hp%']),\n",
       " ('Alencia', ['Hp%', 'Crit Rate%', 'Crit Damage%', 'Speed', 'Effectiveness%'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the 0th element\n",
    "extracted_character_list[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The element is a tuple, containing the character name in the $0th$ index and a list of substats of varying size in the $1st$ index. The important to keep in mind is the method in which I write the substats into my `.csv` file, since they can't be separated by commas. I decided to solve this issue by concatenating all the substats together with the `|` character; initially considered using a whitespace but there are substats with whitespace in its name, like \"Effect Resistance%\".\n",
    "\n",
    "As for the format of the `.csv` file, it should be quite simple:\n",
    "\n",
    "`name`, `substats`\n",
    "\n",
    "Below, I proceed to perfrom the write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(extracted_list):\n",
    "    # open a destination file to write into\n",
    "    e7_unit_data = open(\"e7_unit_data.csv\", \"w\")\n",
    "\n",
    "    # write the headers\n",
    "    e7_unit_data.write(\"name, substats\\n\")\n",
    "\n",
    "    # loop through each element in extracted list\n",
    "    for element in extracted_list:\n",
    "        # 0th element is the unit name\n",
    "        unit_name = element[0]\n",
    "\n",
    "        # 1st element is the unit substats\n",
    "        unit_substats = \"\" # empty string to start concatenation\n",
    "        for substat in element[1]:\n",
    "            unit_substats = unit_substats + substat + \"|\"\n",
    "        # with the way my for loop works, the string will end with \"|\", remove the last character\n",
    "        unit_substats = unit_substats[:-1]\n",
    "\n",
    "        # write unit_name and unit_substats into file\n",
    "        unit_tuple = unit_name + \",\" + unit_substats + \"\\n\"\n",
    "        e7_unit_data.write(unit_tuple)\n",
    "\n",
    "    e7_unit_data.close() # close the destination file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `write_data` implemented, I cal lthe function to begin the write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data into a .csv\n",
    "write_data(extracted_character_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From perusing through the `e7_unit_data.csv` file, the write appears to be successful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
